
import json
import subprocess
import time
import re
import numpy as np
import requests
import pymorphy2

INPUT_FILE = "27.json"
OUTPUT_FILE = "27_cleared.json"
RUBRIC_CONTEXT_PATH = "rubric_context_terms.json"
MANUAL_BLACKLIST = "manual_blacklist.json"
MANUAL_WHITELIST = "manual_whitelist.json"
OLLAMA_MODEL = "nous-hermes2"
SIMILARITY_THRESHOLD = 0.35
MAX_TERMS_IN_PROMPT = 300
MIN_TERMS_THRESHOLD = 1

morph = pymorphy2.MorphAnalyzer()

def normalize_term(term):
    words = re.findall(r'\w+', term.lower())
    return ' '.join(morph.parse(w)[0].normal_form for w in words)

def cosine_similarity(a, b):
    return float(np.dot(a, b))

def run_ollama_with_retries(prompt: str, retries: int = 3, pause: float = 1.5) -> dict:
    for attempt in range(retries):
        try:
            result = subprocess.run(
                ["ollama", "run", OLLAMA_MODEL],
                input=prompt.encode("utf-8"),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=90
            )
            output = result.stdout.decode("utf-8", errors="ignore")
            match = re.search(r'{.*}', output, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group())
                except json.JSONDecodeError:
                    pass
            print(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt+1} –Ω–µ—É–¥–∞—á–Ω–∞...")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ø—ã—Ç–∫–∏ {attempt+1}: {e}")
        time.sleep(pause)
    return {}

PROMPT_TEMPLATE = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–º —Ä—É–±—Ä–∏–∫–∞–º –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å –æ—Ç 5 –¥–æ 15 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—é—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä—É–±—Ä–∏–∫–∏.

üìå –ù–∞–∑–≤–∞–Ω–∏–µ —Ä—É–±—Ä–∏–∫–∏: "{cipher} ‚Äî {title}"
üîó –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è —Ä—É–±—Ä–∏–∫–∞: "{parent_title}"

–í–æ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤: 
{terms}

‚ùó –£–¥–∞–ª–∏ –∏–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ, –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞.

–û—Ç–≤–µ—Ç –≤–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ –≤ JSON-—Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "cipher": "{cipher}",
  "terms": ["—Ç–µ—Ä–º–∏–Ω1", "—Ç–µ—Ä–º–∏–Ω2", "..."]
}}
"""

def prefilter_terms(terms, cipher, blacklist, whitelist):
    cleaned = []
    norm_blacklist = set(normalize_term(t) for t in blacklist.get(cipher, []))
    norm_blacklist |= set(normalize_term(t) for t in blacklist.get("__global__", []))
    norm_whitelist = set(normalize_term(t) for t in whitelist.get(cipher, []))
    for term in terms:
        norm = normalize_term(term)
        if norm in norm_blacklist:
            continue
        if norm in norm_whitelist:
            cleaned.append(term)
            continue
        cleaned.append(term)
    return cleaned

def run_ollama_with_batches(prompt_prefix, term_chunks, cipher):
    all_terms = set()
    for idx, chunk in enumerate(term_chunks):
        prompt = prompt_prefix.format(terms=', '.join(chunk))
        response = run_ollama_with_retries(prompt)
        if response and "terms" in response:
            new_terms = response["terms"]
            all_terms.update(new_terms)
        else:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM –Ω–∞ —á–∞—Å—Ç–∏ {idx+1} –¥–ª—è {cipher}")
    return all_terms

def process_rubric_node(node, blacklist, whitelist, rubric_context_map, global_blacklist):
    cipher = node.get('cipher')
    title = node.get('title')
    parent_title = "–ü–∏—â–µ–≤–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ"
    original_terms = node.get('termList', [])

    terms = [t['content'] for t in original_terms]
    if not terms:
        node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for child in node.get("children", [])]
        return node

    terms = prefilter_terms(terms, cipher, blacklist, whitelist)

    print(f"–†—É–±—Ä–∏–∫–∞ {cipher}, —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(terms)}")

    term_chunks = [terms[i:i + MAX_TERMS_IN_PROMPT] for i in range(0, len(terms), MAX_TERMS_IN_PROMPT)]
    all_llm_terms = set()

    for idx, chunk in enumerate(term_chunks):
        for attempt in range(8):
            try:
                terms_prompt = ', '.join(chunk)
                prompt = PROMPT_TEMPLATE.format(cipher=cipher, title=title, parent_title=parent_title,
                                                terms=terms_prompt)
                result = run_ollama_with_retries(prompt)
                if result and "terms" in result:
                    all_llm_terms.update(result["terms"])
                    break
                else:
                    print(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: LLM –Ω–µ –≤–µ—Ä–Ω—É–ª terms –¥–ª—è —á–∞—Å—Ç–∏ {idx + 1} —Ä—É–±—Ä–∏–∫–∏ {cipher}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM –Ω–∞ —á–∞—Å—Ç–∏ {idx + 1}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {e}")
            time.sleep(1.0)

    if not all_llm_terms:
        print(f"‚ùå LLM –Ω–µ —Å–º–æ–≥ –≤–µ—Ä–Ω—É—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –¥–ª—è {cipher}")
        node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for
                            child in node.get("children", [])]
        return node

    llm_terms = set(normalize_term(t) for t in all_llm_terms)
    manual_blacklist = set(normalize_term(t) for t in blacklist.get(cipher, []))
    manual_whitelist = set(normalize_term(t) for t in whitelist.get(cipher, []))
    blacklist_terms = llm_terms | manual_blacklist | global_blacklist

    filtered_terms = [
        t for t in original_terms
        if normalize_term(t['content']) not in blacklist_terms or normalize_term(t['content']) in manual_whitelist
    ]

    if len(filtered_terms) < MIN_TERMS_THRESHOLD:
        print(f"‚ö†Ô∏è –û—Ç–∫–∞—Ç ‚Äî —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è {cipher}")
        node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for
                            child in node.get("children", [])]
        return node

    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ollama: {len(filtered_terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤")

    rubric_context = rubric_context_map.get(cipher, [])
    term_scores = []

    try:
        emb_r = requests.post("http://localhost:8001/embedding/rubric", json={"title": title, "terms": rubric_context},
                              timeout=5).json()["embedding"]
        emb_r = np.array(emb_r)
        threshold = 0.76 if len(filtered_terms) > 250 else 0.7 if len(filtered_terms) > 150 else 0.6

        term_batches = [filtered_terms[i:i + 300] for i in range(0, len(filtered_terms), 300)]
        for batch in term_batches:
            payload = [{"term": t["content"], "title": title, "context": rubric_context} for t in batch]
            try:
                resp = requests.post("http://localhost:8001/embedding/batch", json=payload, timeout=15)
                term_embeddings = [np.array(d["embedding"]) for d in resp.json()]
                for t, vec in zip(batch, term_embeddings):
                    sim = cosine_similarity(emb_r, vec)
                    if sim >= threshold:
                        term_scores.append((t, sim))
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ relevance-embedding {cipher}: {e}")
                continue
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ embedding –¥–ª—è {cipher}: {e}")
        node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for
                            child in node.get("children", [])]
        return node

    if not term_scores:
        print(f"‚ö†Ô∏è –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã —É–¥–∞–ª–µ–Ω—ã –≤ {cipher} ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—é –æ—Ä–∏–≥–∏–Ω–∞–ª")
        node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for
                            child in node.get("children", [])]
        return node

    term_scores.sort(key=lambda x: -x[1])
    TOP_N_TERMS = 80
    relevant_terms = [t for (t, _) in term_scores[:TOP_N_TERMS]]

    print(
        f"‚ÑπÔ∏è {cipher}: {len(original_terms)} ‚Üí –ø–æ—Å–ª–µ LLM: {len(filtered_terms)} ‚Üí –ø–æ—Å–ª–µ relevance: {len(relevant_terms)} –ª—É—á—à–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (max={TOP_N_TERMS})")

    node["termList"] = relevant_terms
    node["children"] = [process_rubric_node(child, blacklist, whitelist, rubric_context_map, global_blacklist) for child
                        in node.get("children", [])]
    return node


def main():
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    with open(MANUAL_BLACKLIST, "r", encoding="utf-8") as f:
        blacklist = json.load(f)
    with open(MANUAL_WHITELIST, "r", encoding="utf-8") as f:
        whitelist = json.load(f)
    with open(RUBRIC_CONTEXT_PATH, "r", encoding="utf-8") as f:
        rubric_context_map = json.load(f)

    global_blacklist = set(normalize_term(t) for t in blacklist.get("__global__", []))

    cleaned = process_rubric_node(data, blacklist, whitelist, rubric_context_map, global_blacklist)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
